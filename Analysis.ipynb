{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initial setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dateutil\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import simplejson as json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded env vars\n"
     ]
    }
   ],
   "source": [
    "with open('secrets.txt', 'r') as f:\n",
    "    env = json.load(f)\n",
    "print('Loaded env vars')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Quick tokenization and sentiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "from nltk.tokenize import TweetTokenizer\n",
    "tweet_tokenizer = TweetTokenizer()\n",
    "\n",
    "tokenized_train_tweets = [(tweet_tokenizer.tokenize(t),l) for t,l in train_tweets]\n",
    "\n",
    "print(len(tokenized_train_tweets))\n",
    "print(tokenized_train_tweets[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "nltk.download('twitter_samples')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "from nltk.corpus import twitter_samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "pos_tweets = [(t, 'pos') for t in twitter_samples.strings('positive_tweets.json')]\n",
    "neg_tweets = [(t, 'neg') for t in twitter_samples.strings('negative_tweets.json')]\n",
    "print(len(pos_tweets))\n",
    "print(len(neg_tweets))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "random.shuffle(pos_tweets)\n",
    "random.shuffle(neg_tweets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "train_pos_tweets = pos_tweets[:4500]\n",
    "test_pos_tweets = pos_tweets[4500:]\n",
    "\n",
    "train_neg_tweets = neg_tweets[:4500]\n",
    "test_neg_tweets = neg_tweets[4500:]\n",
    "\n",
    "train_tweets = train_pos_tweets + train_neg_tweets\n",
    "test_tweets = test_pos_tweets + test_neg_tweets\n",
    "\n",
    "print(len(train_tweets))\n",
    "print(len(test_tweets))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "from nltk.sentiment import SentimentAnalyzer\n",
    "\n",
    "sentim_analyzer = SentimentAnalyzer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "all_words = sentim_analyzer.all_words(tokenized_train_tweets)\n",
    "print(len(all_words))\n",
    "print(all_words[0:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "from nltk.sentiment.util import extract_unigram_feats\n",
    "\n",
    "unigram_feats = sentim_analyzer.unigram_word_feats(all_words, min_freq=4)\n",
    "print(len(unigram_feats))\n",
    "print(unigram_feats[0:20])\n",
    "sentim_analyzer.add_feat_extractor(extract_unigram_feats, unigrams=unigram_feats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "training_set = sentim_analyzer.apply_features(train_tweets)\n",
    "test_set = sentim_analyzer.apply_features(test_tweets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "from nltk.classify import NaiveBayesClassifier\n",
    "\n",
    "trainer = NaiveBayesClassifier.train\n",
    "classifier = sentim_analyzer.train(trainer, training_set)\n",
    "for key, value in sorted(sentim_analyzer.evaluate(test_set).items()):\n",
    "    print('{0}: {1}'.format(key, value))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# t = 'RT @RedditBTC: Russia plans $10 Billion Bitcoin investment... so it begins. https://t.co/vSSeeZWG9s'\n",
    "t = \"i don't get bitcoin\"\n",
    "classifier.classify(sentim_analyzer.extract_features(t))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# import twython\n",
    "import nltk.sentiment.util\n",
    "from nltk.classify import NaiveBayesClassifier\n",
    "\n",
    "trainer = NaiveBayesClassifier.train\n",
    "\n",
    "# ans = nltk.sentiment.util.demo_tweets(trainer, output='output.txt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Retrieving tweets with tweepy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "import tweepy\n",
    "from tweepy import Cursor\n",
    "\n",
    "auth = tweepy.OAuthHandler(env[tw_oauth_key], env[tw_oauth_secret])\n",
    "auth.set_access_token(env[tw_token_key], env[tw_token_secret])\n",
    "\n",
    "api = tweepy.API(auth)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "bitcoin_tweets = api.search('bitcoin', lang='en', count=100)\n",
    "print(len(bitcoin_tweets))\n",
    "print([t.text for t in bitcoin_tweets[0:5]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "len(bitcoin_tweets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "cursor = Cursor(api.search, q='bitcoin', lang='en', count=100, tweet_mode='extended')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "tlist = []\n",
    "\n",
    "for page in cursor.pages(15):\n",
    "    tlist.extend(page)\n",
    "\n",
    "print(len(tlist))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "tweets = [t.full_text for t in tlist]\n",
    "print(len(tweets))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Using textblob for quick polarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "from textblob import TextBlob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "for t in tweets[0:50]:\n",
    "    print('Tweet:\\n' + t)\n",
    "    blob = TextBlob(t)\n",
    "    sents = blob.sentences\n",
    "    if len(sents) >= 1:\n",
    "        sent = sents[0]\n",
    "        print(sent.sentiment)\n",
    "    print('')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Retrieving tweets from ElasticSearch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from elasticsearch import Elasticsearch\n",
    "from elasticsearch_dsl import Search\n",
    "from elasticsearch_dsl import Document, Date, Text, Integer, Float\n",
    "from elasticsearch_dsl.connections import connections\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a model for tweets stored in the elasticsearch instance:\n",
    "\n",
    "from elasticsearch_dsl import Document, Date, Text, Integer, Float\n",
    "\n",
    "class ESTweet(Document):\n",
    "    created_at = Date()\n",
    "    stored_at = Date()\n",
    "    full_text = Text(analyzer='snowball')\n",
    "    subjectivity = Float()\n",
    "    polarity = Float()\n",
    "    author_id = Integer()\n",
    "    author_followers = Integer()\n",
    "\n",
    "    class Index:\n",
    "        name = 'tweets'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# env vars are loaded at the beginning of the file.\n",
    "client = Elasticsearch(hosts=[env['es_endpoint']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get all tweets from Elasticsearch instance (dangerous without a limit!):\n",
    "\n",
    "MAX_TWEETS = 1000\n",
    "\n",
    "search = Search(index='tweets').using(client)\\\n",
    "            .query(\"match_all\")\\\n",
    "            .sort({'created_at': {'order': 'desc'}})[:MAX_TWEETS]\n",
    "results = search.execute() # results not used at the moment\n",
    "\n",
    "tweet_hits = []\n",
    "for hit in search:\n",
    "    tweet_hits.append(hit)\n",
    "len(tweet_hits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweet_hits[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Not currently used:\n",
    "# tweet_dicts = [t.to_dict() for t in tweets]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hitlist_to_dataframe(hit_list):\n",
    "    \"\"\"\n",
    "    Transform tweet hitlist (from Elasticsearch) to a dataframe\n",
    "    \"\"\"\n",
    "    id_strs = []\n",
    "    created_ats = []\n",
    "    full_texts = []\n",
    "    ndropped = 0\n",
    "    for hit in hit_list:\n",
    "        id_str = str(hit.meta.id)\n",
    "        \n",
    "        if 'created_at' in hit and 'full_text' in hit:\n",
    "            id_strs.append(id_str)\n",
    "            created_ats.append(dateutil.parser.parse(hit['created_at']))\n",
    "            full_texts.append(hit['full_text'])\n",
    "        else:\n",
    "            ndropped += 1\n",
    "    df = pd.DataFrame({'id_str': id_strs, 'created_at': created_ats, 'full_text': full_texts})\n",
    "    print('ndropped = {}'.format(ndropped))\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_tweets = hitlist_to_dataframe(tweet_hits)\n",
    "tweet_hits = None # ready for garbage collection\n",
    "\n",
    "df_tweets.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_tweets.set_index(df_tweets['created_at'])\n",
    "df_tweets = df_tweets.drop(columns=['id_str', 'created_at'])\n",
    "df_tweets.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Show a wordcloud"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from wordcloud import WordCloud"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile the full corpus for a word cloud:\n",
    "\n",
    "corpus = ''\n",
    "for text in df_tweets['full_text']:\n",
    "    corpus += ' '.join(tweet_tokens(text))\n",
    "print(len(corpus))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wordcloud = WordCloud(width=800, height=450, random_state=21, max_font_size=110).generate(corpus)\n",
    "\n",
    "plt.figure(figsize=(10, 7))\n",
    "plt.imshow(wordcloud, interpolation=\"bilinear\")\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NLP on tweets using Sentiment Analysis Dataset (SAD)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1: Load the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import nltk.tokenize\n",
    "import sklearn.feature_extraction.text\n",
    "import sklearn.model_selection\n",
    "import sklearn.naive_bayes\n",
    "import sklearn.pipeline\n",
    "from sklearn import base"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sad = pd.read_csv('Sentiment Analysis Dataset.csv', error_bad_lines=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sad.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sad.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels_sad = df_sad['Sentiment']\n",
    "labels_sad.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sad = df_sad.drop(columns=['ItemID', 'Sentiment', 'SentimentSource'])\n",
    "df_sad.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2: A first pass at cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Custom nltk transformer for cleaning and tokenizing tweets:\n",
    "\n",
    "class TwitterPreprocessor(base.BaseEstimator, base.TransformerMixin):\n",
    "    def __init__(self):\n",
    "        self.tw_tokenizer = nltk.tokenize.TweetTokenizer(\n",
    "            strip_handles=True, reduce_len=True, preserve_case=False)\n",
    "    \n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "    \n",
    "    def transform(self, X, y=None):\n",
    "        ans = X.copy()\n",
    "        ans['SentimentText'] = ans['SentimentText'].apply(self._prepare_tweet)\n",
    "        return ans\n",
    "        \n",
    "    def _prepare_tweet(self, tweet):\n",
    "        tokens = self.tw_tokenizer.tokenize(tweet)\n",
    "        clean_tweet = ' '.join([token for token in tokens\n",
    "                                if len(token) > 1\n",
    "                                and not token.startswith('http')])\n",
    "        return clean_tweet\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tw_preprocessor = TwitterPreprocessor()\n",
    "df2 = tw_preprocessor.transform(df_sad)\n",
    "df2.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# min_df is chosen by off-hand estimate. we should search over it.\n",
    "# try bigrams ASAP\n",
    "tfidf_vectorizer = sklearn.feature_extraction.text.TfidfVectorizer(\n",
    "    ngram_range=(1,1), min_df=100\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raise Exception('Caution: this is slow! Comment this exception if you are sure you want to continue')\n",
    "df3 = tfidf_vectorizer.fit_transform(df2['SentimentText'])\n",
    "print(df3.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(df3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_vectorizer.vocabulary_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reduce the serialized file size\n",
    "tfidf_vectorizer.stop_words_ = None\n",
    "tfidf_vectorizer.vocabulary_ = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.linspace(0,10,11)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mnb_estimator = sklearn.naive_bayes.MultinomialNB()\n",
    "\n",
    "# TODO:\n",
    "# param_grid = {\n",
    "#     'tfidf_vectorizer__min_df': linspace(0,)\n",
    "# }\n",
    "\n",
    "grid_search_cv = sklearn.model_selection.GridSearchCV(\n",
    "    mnb_estimator, param_grid={'alpha': [0.1, 0.5, 1.0, 10.0]}, cv=5\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_search_cv.fit(df3, labels_sad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_estimator = grid_search_cv.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_estimator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dill\n",
    "\n",
    "dill.dump(tw_preprocessor, open('tw_preprocessor.dill', 'wb'))\n",
    "dill.dump(tfidf_vectorizer, open('tfidf_vectorizer.dill', 'wb'))\n",
    "dill.dump(best_estimator, open('best_mnb_estimator.dill', 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mnb_estimator = sklearn.naive_bayes.MultinomialNB()\n",
    "\n",
    "grid_search_cv = sklearn.model_selection.GridSearchCV(\n",
    "    mnb_estimator, param_grid={'alpha': [0.1, 0.5, 1.0, 10.0]}, cv=5\n",
    ")\n",
    "\n",
    "pipe = sklearn.pipeline.Pipeline([\n",
    "    ('tw_preprocessor', tw_preprocessor),\n",
    "    ('tfidf_vectorizer', tfidf_vectorizer),\n",
    "    ('grid_search_cv', grid_search_cv)\n",
    "])\n",
    "pipe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mnb_estimator.score(df3, labels_sad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe = sklearn.pipeline.Pipeline([\n",
    "    ('tw_preprocessor', tw_preprocessor),\n",
    "    ('tfidf_vectorizer', tfidf_vectorizer),\n",
    "    ('estimator', estimator)\n",
    "])\n",
    "pipe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_word_cloud(df, column_name='full_text'):\n",
    "    corpus = ''\n",
    "    for text in df[column_name]:\n",
    "        corpus += text\n",
    "        corpus += ' '\n",
    "\n",
    "    cloud = wordcloud.WordCloud(\n",
    "        width=800, height=450, random_state=21, max_font_size=110, background_color='white'\n",
    "    ).generate(corpus)\n",
    "\n",
    "    plt.figure(figsize=(8, 5))\n",
    "    plt.imshow(cloud, interpolation=\"bilinear\")\n",
    "    plt.axis('off')\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "build_word_cloud(df2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "estimator.predict(df3[0:100])\n",
    "\n",
    "tab = pd.DataFrame()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels_sad[:100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tab = df2[:100]\n",
    "tab['labels_sad'] = labels_sad[:100]\n",
    "tab['predicted'] = estimator.predict(df3[:100])\n",
    "tab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "example = pd.DataFrame({'SentimentText': ['bitcoin is awesome and it will grow this year']})\n",
    "print(example)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe.predict(example)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bokeh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import bokeh\n",
    "from bokeh.plotting import figure, show\n",
    "from bokeh.io import output_notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_notebook()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_tweets.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_tb_polarity(tweet):\n",
    "    blob = textblob.TextBlob(tweet)\n",
    "    sents = blob.sentences\n",
    "    if len(sents) >= 1:\n",
    "        sent = sents[0]\n",
    "        return float(sent.sentiment.polarity)\n",
    "    else:\n",
    "        return np.NaN\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_tweets['tb_polarity'] = df_tweets['full_text'].apply(get_tb_polarity)\n",
    "df_tweets.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_tweets.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_tweets['tb_polarity'].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s_polarity = pd.Series(data=df_tweets['tb_polarity'].values, index=df_tweets['created_at'].values)\n",
    "s_polarity = s_polarity.sort_index()\n",
    "len(s_polarity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s_polarity = s_polarity.resample('H').mean()\n",
    "print(len(s_polarity))\n",
    "s_polarity.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import bokeh.models\n",
    "\n",
    "p = figure(\n",
    "    plot_width=800, plot_height=450,\n",
    "    tools=\"\",\n",
    "    x_axis_label='time', y_axis_label='polarity',\n",
    "    x_axis_type='datetime'\n",
    ")\n",
    "\n",
    "# Format background colors:\n",
    "low_box = bokeh.models.BoxAnnotation(top=0, fill_alpha=0.1, fill_color='red')\n",
    "high_box = bokeh.models.BoxAnnotation(bottom=0, fill_alpha=0.1, fill_color='green')\n",
    "p.add_layout(low_box)\n",
    "p.add_layout(high_box)\n",
    "\n",
    "# Format gridlines:\n",
    "p.xgrid[0].grid_line_color=None\n",
    "p.ygrid[0].grid_line_alpha=0.5\n",
    "\n",
    "# Format view range:\n",
    "p.y_range = bokeh.models.Range1d(-1.0, 1.0)\n",
    "\n",
    "# Prepare data:\n",
    "cd_source = bokeh.models.ColumnDataSource({'timestamp': s_polarity.index, 'polarity': s_polarity})\n",
    "\n",
    "p.line('timestamp', 'polarity', source=cd_source, line_width=2)\n",
    "p.circle('timestamp', 'polarity', source=cd_source, fill_color=\"white\", size=2)\n",
    "\n",
    "p.add_tools(bokeh.models.HoverTool(\n",
    "    tooltips=[\n",
    "        ('time', '@timestamp{%F %H:%M}'),\n",
    "        (\"polarity\", \"@polarity{+0.00}\")\n",
    "    ],\n",
    "    formatters={'timestamp': 'datetime', 'polarity': 'numeral'},\n",
    "    mode='vline'\n",
    "))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show(p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
